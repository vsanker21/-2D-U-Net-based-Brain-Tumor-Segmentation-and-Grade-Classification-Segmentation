{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Brain Tumor Segmentation and Grade Classification\n",
    "\n",
    "This notebook implements:\n",
    "1. **2D U-Net** for multi-modal brain tumor segmentation (T1, T2, T1+T2) - slice-based processing\n",
    "2. **Grade Classification** (HGG vs LGG) using 3D CNN for full-volume classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                           roc_auc_score, accuracy_score, f1_score,\n",
    "                           precision_score, recall_score)\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "DATA_DIR = Path(r\"E:\\Brain Tumor Segmentation\\archive\\3D Slices Sorted\")\n",
    "GRADE_MAPPING_FILE = Path(r\"E:\\Brain Tumor Segmentation\\archive\\BraTS2020_training_data\\content\\data\\name_mapping.csv\")\n",
    "\n",
    "# Training parameters\n",
    "SEGMENTATION_EPOCHS = 30\n",
    "CLASSIFICATION_EPOCHS = 30\n",
    "BATCH_SIZE_SEG = 16  # Larger batch size for 2D slices\n",
    "BATCH_SIZE_CLS = 4\n",
    "LEARNING_RATE = 1e-3  # Higher learning rate for 2D U-Net\n",
    "TEST_SIZE = 0.2  # 80% train, 20% test\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architectures\n",
    "\n",
    "### 3.1 Segmentation Model: 2D U-Net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet2D(nn.Module):\n",
    "    \"\"\"2D U-Net optimized for slice-based brain tumor segmentation.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=1, num_classes=4, base_features=32):\n",
    "        super(UNet2D, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = self._conv_block(in_channels, base_features)\n",
    "        self.enc2 = self._conv_block(base_features, base_features * 2)\n",
    "        self.enc3 = self._conv_block(base_features * 2, base_features * 4)\n",
    "        self.enc4 = self._conv_block(base_features * 4, base_features * 8)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = self._conv_block(base_features * 8, base_features * 16)\n",
    "        \n",
    "        # Decoder\n",
    "        self.up4 = nn.ConvTranspose2d(base_features * 16, base_features * 8, 2, 2)\n",
    "        self.dec4 = self._conv_block(base_features * 16, base_features * 8)\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(base_features * 8, base_features * 4, 2, 2)\n",
    "        self.dec3 = self._conv_block(base_features * 8, base_features * 4)\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(base_features * 4, base_features * 2, 2, 2)\n",
    "        self.dec2 = self._conv_block(base_features * 4, base_features * 2)\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(base_features * 2, base_features, 2, 2)\n",
    "        self.dec1 = self._conv_block(base_features * 2, base_features)\n",
    "        \n",
    "        # Final classification\n",
    "        self.final = nn.Conv2d(base_features, num_classes, kernel_size=1)\n",
    "        \n",
    "        # Max pooling\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout2d(0.1)\n",
    "        \n",
    "    def _conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        e4 = self.enc4(self.pool(e3))\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(self.pool(e4))\n",
    "        \n",
    "        # Decoder\n",
    "        d4 = self.up4(b)\n",
    "        d4 = torch.cat([d4, e4], dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "        \n",
    "        d3 = self.up3(d4)\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        \n",
    "        d2 = self.up2(d3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        \n",
    "        d1 = self.up1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "        \n",
    "        # Final prediction\n",
    "        out = self.final(d1)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Classification Model: Grade Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TumorGradeClassifier(nn.Module):\n",
    "    \"\"\"Brain tumor grade classifier using transfer learning from segmentation model.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=2, dropout_rate=0.5):\n",
    "        super(TumorGradeClassifier, self).__init__()\n",
    "        \n",
    "        # Feature extraction layers (inspired by segmentation model)\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv3d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(2),  # 240x240x155 -> 120x120x77\n",
    "            \n",
    "            nn.Conv3d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(2),  # 120x120x77 -> 60x60x38\n",
    "            \n",
    "            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(2),  # 60x60x38 -> 30x30x19\n",
    "            \n",
    "            nn.Conv3d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool3d((4, 4, 4))  # Global average pooling\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 4 * 4 * 4, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        output = self.classifier(features)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Dice Loss for multi-class segmentation (2D).\"\"\"\n",
    "    \n",
    "    def __init__(self, smooth=1e-5):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        pred = F.softmax(pred, dim=1)\n",
    "        target_one_hot = F.one_hot(target, num_classes=pred.shape[1]).permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        intersection = (pred * target_one_hot).sum(dim=(2, 3))\n",
    "        union = pred.sum(dim=(2, 3)) + target_one_hot.sum(dim=(2, 3))\n",
    "        \n",
    "        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Combined Dice and Cross-Entropy Loss.\"\"\"\n",
    "    \n",
    "    def __init__(self, dice_weight=0.5, ce_weight=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.dice_loss = DiceLoss()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.dice_weight = dice_weight\n",
    "        self.ce_weight = ce_weight\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        dice = self.dice_loss(pred, target)\n",
    "        ce = self.ce_loss(pred, target)\n",
    "        return self.dice_weight * dice + self.ce_weight * ce\n",
    "\n",
    "class WeightedFocalLoss(nn.Module):\n",
    "    \"\"\"Weighted Focal Loss for class imbalance.\"\"\"\n",
    "    \n",
    "    def __init__(self, class_weights, alpha=1, gamma=2):\n",
    "        super(WeightedFocalLoss, self).__init__()\n",
    "        self.class_weights = torch.FloatTensor(class_weights)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        ce_loss = F.cross_entropy(pred, target, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        # Apply class weights\n",
    "        if self.class_weights.device != pred.device:\n",
    "            self.class_weights = self.class_weights.to(pred.device)\n",
    "        weights = self.class_weights[target]\n",
    "        weighted_focal_loss = weights * focal_loss\n",
    "        \n",
    "        return weighted_focal_loss.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SliceBasedDataset(Dataset):\n",
    "    \"\"\"Memory-efficient slice-based dataset for 2D brain tumor segmentation.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, patient_ids, modalities=['T1'], transform=None):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.patient_ids = patient_ids\n",
    "        self.modalities = modalities\n",
    "        self.transform = transform\n",
    "        self.samples = self._create_samples()\n",
    "        \n",
    "    def _create_samples(self):\n",
    "        \"\"\"Create list of samples (patient_id, slice_index).\"\"\"\n",
    "        samples = []\n",
    "        for patient_id in self.patient_ids:\n",
    "            # Load one image to get dimensions\n",
    "            img_path = self.data_dir / f\"BraTS20_Training_{patient_id:03d}_{self.modalities[0]}.nii.gz\"\n",
    "            if img_path.exists():\n",
    "                img = nib.load(img_path).get_fdata()\n",
    "                num_slices = img.shape[2]\n",
    "                \n",
    "                # Create samples for every slice\n",
    "                for i in range(num_slices):\n",
    "                    samples.append((patient_id, i))\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        patient_id, slice_idx = self.samples[idx]\n",
    "        \n",
    "        # Load image modalities for the specific slice\n",
    "        images = []\n",
    "        for modality in self.modalities:\n",
    "            img_path = self.data_dir / f\"BraTS20_Training_{patient_id:03d}_{modality}.nii.gz\"\n",
    "            if img_path.exists():\n",
    "                img = nib.load(img_path).get_fdata()\n",
    "                # Extract single slice\n",
    "                img_slice = img[:, :, slice_idx]\n",
    "                # Normalize\n",
    "                img_slice = (img_slice - img_slice.min()) / (img_slice.max() - img_slice.min() + 1e-8)\n",
    "                images.append(img_slice)\n",
    "            else:\n",
    "                images.append(np.zeros((240, 240)))\n",
    "        \n",
    "        # Stack modalities\n",
    "        image = np.stack(images, axis=0)  # Shape: (n_modalities, H, W)\n",
    "        \n",
    "        # Load segmentation mask for the same slice\n",
    "        mask = self.load_segmentation_mask(patient_id, slice_idx)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image, mask = self.transform(image, mask)\n",
    "        \n",
    "        return torch.FloatTensor(image), torch.LongTensor(mask)\n",
    "    \n",
    "    def load_segmentation_mask(self, patient_id, slice_idx):\n",
    "        \"\"\"Load segmentation mask for specific slice.\"\"\"\n",
    "        mask_path = self.data_dir / \"masks\" / f\"BraTS20_Training_{patient_id:03d}_mask.nii.gz\"\n",
    "        if mask_path.exists():\n",
    "            mask = nib.load(mask_path).get_fdata()\n",
    "            mask_slice = mask[:, :, slice_idx]\n",
    "            return mask_slice.astype(np.uint8)\n",
    "        else:\n",
    "            return np.zeros((240, 240), dtype=np.uint8)\n",
    "\n",
    "class BrainTumorClassificationDataset(Dataset):\n",
    "    \"\"\"Dataset for brain tumor grade classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, grade_mapping, patient_ids, transform=None):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.grade_mapping = grade_mapping\n",
    "        self.patient_ids = patient_ids\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.patient_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = self.patient_ids[idx]\n",
    "        \n",
    "        # Get the grade for this patient\n",
    "        patient_name = f\"BraTS20_Training_{patient_id:03d}\"\n",
    "        grade = self.grade_mapping[patient_name]\n",
    "        label = 1 if grade == 'HGG' else 0  # HGG=1, LGG=0\n",
    "        \n",
    "        # Load T2 image (best modality from segmentation)\n",
    "        img_path = self.data_dir / f\"BraTS20_Training_{patient_id:03d}_T2.nii.gz\"\n",
    "        \n",
    "        if img_path.exists():\n",
    "            img = nib.load(img_path).get_fdata()\n",
    "            # Normalize the image\n",
    "            img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "            \n",
    "            # Convert to tensor and add channel dimension\n",
    "            img = torch.FloatTensor(img).unsqueeze(0)  # Shape: (1, H, W, D)\n",
    "            \n",
    "            # Apply transforms if provided\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            \n",
    "            return img, torch.LongTensor([label])\n",
    "        else:\n",
    "            return torch.zeros((1, 240, 240, 155)), torch.LongTensor([label])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dice_scores(pred, target, num_classes=4):\n",
    "    \"\"\"Calculate Dice score for each class.\"\"\"\n",
    "    pred = F.softmax(pred, dim=1)\n",
    "    pred_classes = torch.argmax(pred, dim=1)\n",
    "    \n",
    "    dice_scores = []\n",
    "    for i in range(num_classes):\n",
    "        pred_i = (pred_classes == i).float()\n",
    "        target_i = (target == i).float()\n",
    "        \n",
    "        intersection = (pred_i * target_i).sum()\n",
    "        union = pred_i.sum() + target_i.sum()\n",
    "        \n",
    "        if union > 0:\n",
    "            dice = (2.0 * intersection) / union\n",
    "        else:\n",
    "            dice = torch.tensor(1.0 if pred_i.sum() == 0 else 0.0)\n",
    "        \n",
    "        dice_scores.append(dice.item())\n",
    "    \n",
    "    return dice_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: SEGMENTATION\n",
    "\n",
    "## 7. Data Preparation for Segmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_segmentation_data(data_dir, modalities=['T1'], test_size=0.2, random_state=42):\n",
    "    \"\"\"Prepare dataset for segmentation training (2D slice-based).\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    \n",
    "    # Get all patient IDs\n",
    "    patient_ids = []\n",
    "    for modality in modalities:\n",
    "        modality_files = list(data_dir.glob(f\"*_{modality}.nii.gz\"))\n",
    "        for file in modality_files:\n",
    "            patient_id = int(file.name.split('_')[2])\n",
    "            if patient_id not in patient_ids:\n",
    "                patient_ids.append(patient_id)\n",
    "    \n",
    "    patient_ids.sort()\n",
    "    \n",
    "    # Split data (80% train, 20% test)\n",
    "    train_ids, test_ids = train_test_split(patient_ids, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Create datasets (slice-based for 2D U-Net)\n",
    "    train_dataset = SliceBasedDataset(data_dir, train_ids, modalities)\n",
    "    test_dataset = SliceBasedDataset(data_dir, test_ids, modalities)\n",
    "    \n",
    "    print(f\"Found {len(patient_ids)} patients\")\n",
    "    print(f\"Training samples: {len(train_dataset)} slices\")\n",
    "    print(f\"Test samples: {len(test_dataset)} slices\")\n",
    "    \n",
    "    return train_dataset, test_dataset, train_ids, test_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Segmentation Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_segmentation_model(modalities, train_dataset, test_dataset, epochs=30, batch_size=16, lr=1e-3):\n",
    "    \"\"\"Train segmentation model for given modalities (2D U-Net).\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training 2D U-Net segmentation model for modalities: {modalities}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Initialize model (2D U-Net)\n",
    "    model_name = '+'.join(modalities)\n",
    "    model = UNet2D(in_channels=len(modalities), num_classes=4).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "    criterion = CombinedLoss()\n",
    "    \n",
    "    # Training tracking\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_dice_scores = []\n",
    "    best_dice = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        dice_scores_epoch = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, masks in test_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Calculate Dice scores\n",
    "                dice_scores = calculate_dice_scores(outputs, masks)\n",
    "                dice_scores_epoch.append(dice_scores)\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        avg_dice = np.mean([np.mean(dice) for dice in dice_scores_epoch])\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_dice_scores.append(avg_dice)\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}: Train Loss: {avg_train_loss:.4f}, '\n",
    "              f'Val Loss: {avg_val_loss:.4f}, Val Dice: {avg_dice:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_dice > best_dice:\n",
    "            best_dice = avg_dice\n",
    "            best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    print(f\"\\nBest Dice Score for {modalities}: {best_dice:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'best_model_state': best_model_state,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_dice_scores': val_dice_scores,\n",
    "        'best_dice': best_dice\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define modality combinations to test\n",
    "modality_combinations = [\n",
    "    ['T1'],\n",
    "    ['T2'],\n",
    "    ['T1', 'T2']\n",
    "]\n",
    "\n",
    "# Store results\n",
    "segmentation_results = {}\n",
    "\n",
    "# Train models for each combination\n",
    "for modalities in modality_combinations:\n",
    "    # Prepare data\n",
    "    train_dataset, test_dataset, train_ids, test_ids = prepare_segmentation_data(\n",
    "        DATA_DIR, modalities, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    result = train_segmentation_model(\n",
    "        modalities, train_dataset, test_dataset, \n",
    "        epochs=SEGMENTATION_EPOCHS, batch_size=BATCH_SIZE_SEG, lr=LEARNING_RATE\n",
    "    )\n",
    "    \n",
    "    model_name = '+'.join(modalities)\n",
    "    segmentation_results[model_name] = result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Segmentation Results Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SEGMENTATION RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model_name, results in segmentation_results.items():\n",
    "    print(f\"{model_name}: Best Dice Score = {results['best_dice']:.4f}\")\n",
    "\n",
    "# Find best model\n",
    "best_seg_model = max(segmentation_results.items(), key=lambda x: x[1]['best_dice'])\n",
    "print(f\"\\nBest performing model: {best_seg_model[0]} with Dice Score: {best_seg_model[1]['best_dice']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: CLASSIFICATION\n",
    "\n",
    "## 11. Data Preparation for Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load grade mapping\n",
    "def load_grade_mapping(grade_mapping_file):\n",
    "    \"\"\"Load grade mapping from CSV file.\"\"\"\n",
    "    df = pd.read_csv(grade_mapping_file)\n",
    "    grade_mapping = {}\n",
    "    for _, row in df.iterrows():\n",
    "        patient_name = row['BraTS_2020_subject_ID']\n",
    "        grade = row['Grade']\n",
    "        grade_mapping[patient_name] = grade\n",
    "    return grade_mapping\n",
    "\n",
    "grade_mapping = load_grade_mapping(GRADE_MAPPING_FILE)\n",
    "print(f\"Loaded grade information for {len(grade_mapping)} patients\")\n",
    "\n",
    "# Prepare classification data\n",
    "def prepare_classification_data(data_dir, grade_mapping, test_size=0.2, random_state=42):\n",
    "    \"\"\"Prepare dataset for classification.\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    \n",
    "    # Get all patient IDs with grade information\n",
    "    patient_ids = []\n",
    "    t2_files = list(data_dir.glob(\"*_T2.nii.gz\"))\n",
    "    \n",
    "    for file in t2_files:\n",
    "        try:\n",
    "            patient_id = int(file.name.split('_')[2])\n",
    "            patient_name = f\"BraTS20_Training_{patient_id:03d}\"\n",
    "            \n",
    "            if patient_name in grade_mapping:\n",
    "                patient_ids.append(patient_id)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    patient_ids.sort()\n",
    "    print(f\"Found {len(patient_ids)} patients with grade information\")\n",
    "    \n",
    "    # Get labels for stratification\n",
    "    labels = []\n",
    "    for patient_id in patient_ids:\n",
    "        patient_name = f\"BraTS20_Training_{patient_id:03d}\"\n",
    "        grade = grade_mapping[patient_name]\n",
    "        label = 1 if grade == 'HGG' else 0\n",
    "        labels.append(label)\n",
    "    \n",
    "    # Check class distribution\n",
    "    label_counts = Counter(labels)\n",
    "    print(f\"Class distribution: HGG={label_counts[1]}, LGG={label_counts[0]}\")\n",
    "    \n",
    "    # Stratified split (80% train, 20% test)\n",
    "    train_ids, test_ids, train_labels, test_labels = train_test_split(\n",
    "        patient_ids, labels, test_size=test_size, random_state=random_state, \n",
    "        stratify=labels\n",
    "    )\n",
    "    \n",
    "    return train_ids, test_ids, train_labels, test_labels\n",
    "\n",
    "train_ids, test_ids, train_labels, test_labels = prepare_classification_data(\n",
    "    DATA_DIR, grade_mapping, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "print(f\"Training samples: {len(train_ids)}\")\n",
    "print(f\"Test samples: {len(test_ids)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Classification Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(labels):\n",
    "    \"\"\"Calculate class weights for loss function.\"\"\"\n",
    "    class_counts = Counter(labels)\n",
    "    total_samples = len(labels)\n",
    "    \n",
    "    weights = []\n",
    "    for class_id in sorted(class_counts.keys()):\n",
    "        weight = total_samples / (len(class_counts) * class_counts[class_id])\n",
    "        weights.append(weight)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def create_weighted_sampler(labels):\n",
    "    \"\"\"Create weighted sampler for class imbalance.\"\"\"\n",
    "    class_counts = Counter(labels)\n",
    "    total_samples = len(labels)\n",
    "    \n",
    "    weights = []\n",
    "    for label in labels:\n",
    "        weight = total_samples / (len(class_counts) * class_counts[label])\n",
    "        weights.append(weight)\n",
    "    \n",
    "    return WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "def train_classification_model(train_ids, test_ids, train_labels, test_labels, \n",
    "                               epochs=30, batch_size=4, lr=1e-4):\n",
    "    \"\"\"Train the classification model.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training Brain Tumor Grade Classification Model\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = BrainTumorClassificationDataset(DATA_DIR, grade_mapping, train_ids)\n",
    "    test_dataset = BrainTumorClassificationDataset(DATA_DIR, grade_mapping, test_ids)\n",
    "    \n",
    "    # Create weighted sampler for training\n",
    "    train_sampler = create_weighted_sampler(train_labels)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        sampler=train_sampler,\n",
    "        num_workers=2,\n",
    "        pin_memory=True if device.type == 'cuda' else False\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2,\n",
    "        pin_memory=True if device.type == 'cuda' else False\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = TumorGradeClassifier(num_classes=2, dropout_rate=0.5).to(device)\n",
    "    \n",
    "    # Calculate class weights\n",
    "    class_weights = calculate_class_weights(train_labels)\n",
    "    print(f\"Class weights: {class_weights}\")\n",
    "    \n",
    "    # Optimizer and loss function\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "    criterion = WeightedFocalLoss(class_weights=class_weights, alpha=1, gamma=2)\n",
    "    \n",
    "    # Training tracking\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    best_f1 = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device).squeeze()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device).squeeze()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}: Train Loss: {avg_train_loss:.4f} | '\n",
    "              f'Train Acc: {train_accuracy:.2f}% | Val Loss: {avg_val_loss:.4f} | '\n",
    "              f'Val Acc: {val_accuracy:.2f}% | F1: {f1:.4f}')\n",
    "        \n",
    "        # Save best model based on F1 score\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    print(f\"\\nBest F1 Score: {best_f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'best_model_state': best_model_state,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'best_f1': best_f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classification model\n",
    "classification_result = train_classification_model(\n",
    "    train_ids, test_ids, train_labels, test_labels,\n",
    "    epochs=CLASSIFICATION_EPOCHS, batch_size=BATCH_SIZE_CLS, lr=LEARNING_RATE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Classification Evaluation and Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "def evaluate_classification_model(model, test_ids, grade_mapping):\n",
    "    \"\"\"Comprehensive model evaluation.\"\"\"\n",
    "    test_dataset = BrainTumorClassificationDataset(DATA_DIR, grade_mapping, test_ids)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE_CLS, shuffle=False, num_workers=2)\n",
    "    \n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device).squeeze()\n",
    "            outputs = model(images)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities[:, 1].cpu().numpy())  # HGG probabilities\n",
    "    \n",
    "    # Calculate comprehensive metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "    \n",
    "    try:\n",
    "        auc_roc = roc_auc_score(all_labels, all_probabilities)\n",
    "    except:\n",
    "        auc_roc = 0.0\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auc_roc': auc_roc,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': classification_report(all_labels, all_predictions)\n",
    "    }\n",
    "\n",
    "# Evaluate model\n",
    "final_metrics = evaluate_classification_model(\n",
    "    classification_result['model'], test_ids, grade_mapping\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {final_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {final_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {final_metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score: {final_metrics['f1_score']:.4f}\")\n",
    "print(f\"AUC-ROC: {final_metrics['auc_roc']:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(final_metrics['confusion_matrix'])\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(final_metrics['classification_report'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Final Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL PROJECT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n--- SEGMENTATION RESULTS ---\")\n",
    "for model_name, results in segmentation_results.items():\n",
    "    print(f\"{model_name}: Best Dice Score = {results['best_dice']:.4f}\")\n",
    "\n",
    "best_seg_model = max(segmentation_results.items(), key=lambda x: x[1]['best_dice'])\n",
    "print(f\"\\nBest Segmentation Model: {best_seg_model[0]} (Dice: {best_seg_model[1]['best_dice']:.4f})\")\n",
    "\n",
    "print(\"\\n--- CLASSIFICATION RESULTS ---\")\n",
    "print(f\"Accuracy: {final_metrics['accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {final_metrics['f1_score']:.4f}\")\n",
    "print(f\"AUC-ROC: {final_metrics['auc_roc']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Project 1 Complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
